<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="summary" name="twitter:card"/>
<meta content="https://ngc7292.github.io/images/avatar.png" name="twitter:image"/>
<meta content="Data view for LLM pre-training" name="twitter:title"/>
<meta content="In this blog, we aims to provide a data’s view to research the emergence ability of large language model(LLM),  and provide some point for NLP community to reproduce the GPT-3 or PaLM easily." name="twitter:description"/>
<meta content="@ngc7293" name="twitter:site"/>
<meta content="@ngc7293" name="twitter:creator"/>
<meta content="ngc7293" name="author"/>
<meta content="Hugo 0.78.1" name="generator"/>
<title>Data view for LLM pre-training · ngc7293's blog</title>
<link href="https://ngc7292.github.io/images/favicon.ico" rel="shortcut icon"/>
<link href="https://ngc7292.github.io/css/style.css" rel="stylesheet"/>
<link href="https://ngc7292.github.io/css/highlight.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/gh/ngc7292/live2d-widget@latest/autoload.js"></script>
<link href="https://ngc7292.github.io/css/font-awesome.min.css" rel="stylesheet"/>
<link href="https://ngc7292.github.io/index.xml" rel="alternate" title="ngc7293's blog" type="application/rss+xml"/>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
</head>
<body>
<nav class="main-nav">
<a href="https://ngc7292.github.io/">Home</a>
<a href="https://ngc7292.github.io/posts">Archive</a>
<a href="https://ngc7292.github.io/tags">Tags</a>
<a href="https://ngc7292.github.io/about">About</a>
<a class="cta" href="https://ngc7292.github.io/index.xml">Subscribe</a>
</nav>
<section class="post" id="wrapper">
<article>
<header>
<h1>
                        Data view for LLM pre-training
                    </h1>
<h2 class="headline">
                    Mar 1, 2023 00:00
                    · 411 words
                    · 2 minute read
                      <span class="tags">
<a href="https://ngc7292.github.io/tags/llm">LLM</a>
</span>
</h2>
</header>
<div id="toc">
<nav id="TableOfContents">
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#instruction">Instruction</a></li>
<li><a href="#fundational-model-pretraining-data">Fundational model pretraining data</a></li>
<li><a href="#refenrence">Refenrence</a></li>
</ul>
</nav>
</div>
<section id="post-body">
<p>In this blog, we aims to provide a data’s view to research the emergence ability of large language model(LLM),  and provide some point for NLP community to reproduce the GPT-3 or PaLM easily.</p>
<h2 id="abstract">Abstract</h2>
<p>Large language model have a most powerful to solve the nature language tasks, not only summerization or text classification. Recent research show the LLM have much emergence ability when the model’s size arrive a trade-off line. However, the LLM shown us the emergence model like GPT-3 and PaLM does’t release public, the reproduce of NLP community like OPT, GLM and BLOOM even have the similar size have a long way to reach these.In this post, we aims to get a survey for this fundamental model producing methods to anylisis the reproductions' shortage and provide a good way to reproduce these model so clearly.</p>
<p>大型语言模型具有最强大的解决自然语言任务的能力，而不仅仅是求和或文本分类。最近的研究表明，当模型的大小到达折衷线时，LLM 具有很强的涌现能力。然而，LLM 向我们展示了像 GPT-3 和 PaLM 这样的涌现模型并没有公开发布，像 OPT、GLM 和 BLOOM 这样的 NLP 社区的复制即使有类似的规模也有很长的路要走。在这篇文章中，我们旨在对这种基本的模型制作方法进行调查，以解决复制品的不足，并提供一种将这些模型如此清晰地复制的好方法。</p>
<h2 id="instruction">Instruction</h2>
<p>通用人工智能一直以来都是研究人员的研究热点，并有着前赴后继的研究人员投入到这类研究中。在2022年的时间点中，人工智能已经可以辅助人类进行诸如订票，写文章，作为客服回答顾客的问题等简单的工作，但是通俗意义下，人们认为其距离通用人工智能仍然有一定程度的距离（即便在此期间，openai以及谷歌的研究人员发现极大的自回归语言模型可能会存在推理【COT相关论文】以及自我意识【推特】的行为，但是都没有引起当时研究界的重视）。在2022年ChatGPT发布之后，为研究人员实现强人工智能（通用人工智能）提供了一条可以实现的道路。</p>
<p>ChatGPt是由OpenAI所开发的InstructGPT继续开发的聊天机器人，其在刚刚发布的一个月内就获得了超过一百万的用户数量。人们惊异于chatgpt对于人类语言的理解能力以及对语言的组织能力，对于任意的问题，chatgpt可以精确的识别用户的意图并给出相对较为准确的答案，其强大一图识别能力是之前的人工智能所不具备的。虽然chatgpt具体的细节并未给出，但是从openai给出的信息中我们可以得到几个关键的信息，即大规模预训练语言模型，自微调学习以及人在回路学习。目前一个简单的共识为，针对预训练任务的训练（GPT 系列采用的是自回归语言建模任务）不仅使模型学习了相当多的知识， 同时还学习到了相对基础的推理能力，自微调算法以及人在回路学习方法使模型学会如何识别用户的意图以及如何释放之前学习到的知识。因此，构建该类通用人工智能的技术中，基础模型的构建与后期学习同样重要。</p>
<p>如前所述，大规模预训练语言模型模型是构建此类通用人工智能的基础与前提，但是构建高效高质量的大规模预训练语言模型需要海量的数据，高额的计算量以及充分的技术储备。虽然目前通用人工智能的训练相当火爆，但是并没有文章系统性的调查目前NLP社区中的大规模预训练语言模型的研究现状与进展。因此，本文从数据，架构，以及评测等多个角度较为系统的调查了较为流行的几个大规模预训练语言模型，来汇总目前大规模预训练语言模型相关的从数据预处理，模型架构选择以及最终的大模型的部分性能对比，以分析大规模预训练模型后续的能力初期来源。</p>
<h2 id="fundational-model-pretraining-data">Fundational model pretraining data</h2>
<p>数据是知识的载体，也是是大规模预训练模型训练的基础。数据的质量一定程度上决定大规模预训练模型的效果以及未来将会释放出的突显能力。一个供给给大规模预训练语言模型所使用的语料通常要包含几种基础的属性，包括数量，多样性与高质量。图x中包含几种大规模预训练语言模型所采用的预训练语料的来源，数量，以及所公布的在pile数据集中的困惑度（在一定程度上代表语料质量的高低，但是不绝对）。</p>
<ol>
<li>
<p>数据来源</p>
<p>为了保证数据高质量以及多样性，大部分的预训练语言模型的预训练语料都选择了从互联网中获取相对应的文本数据，根据谷歌公司CEO Eric Schmidt 预计，整个互联网数据数量高达5000 PB<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" role="doc-noteref">1</a></sup>，然而如何获取这一份数据是一项巨大的挑战。受益于互联网爬虫计划common crawl<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" role="doc-noteref">2</a></sup>的开展，研究人员可以更加便利的收集网络中的数据，并将精力集中于数据处理阶段。common crawl是一项开放网络爬虫的数据存储库的开源项目，其爬取并保存了自2013年以来从开放互联网中爬取的各种数据。其主要包括两类，一类是保存的原始数据（warc格式）另一类是简单将html格式中的文本进行抽取出来的文本数据（wet格式）。一般认为，warc格式的数据中保留了网站原本的结构与布局，其内容较为丰富与完善，但是缺点是冗余信息较多，抽取与处理较为复杂，而wet格式是经过简单文本抽取的文本数据，丢失了较多的信息，但是处理起来较为简单，另外xx等人<sup id="fnref:3"><a class="footnote-ref" href="#fn:3" role="doc-noteref">3</a></sup>认为，wet文本可能会讲框架或者布局中的数据掺杂在正文中，这将破坏正文的连贯性与一致性，从而影响模型对于语言建模任务的学习。</p>
<p>虽然common crawl中包含了大量的数据，但是由于其数据量大并且其质量不对等导致对于大规模预训练语言模型的训练具有不可控的影响，因此，之前的几份工作都表示引入了更多的预处理数据，或者针对指定的网站的数据进行定向收集。例如，在Tom etc.<sup id="fnref:4"><a class="footnote-ref" href="#fn:4" role="doc-noteref">4</a></sup>提到，除了预处理common crawl的数据外，他们同样添加了高质量图书数据集Books1，Books2以及维基百科等作为高质量数据的补充，同时也作为其训练质量分类器的训练数据。Pile<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" role="doc-noteref">5</a></sup>除了common crawl收集的数据外，还收集了将近21个站点的数据，其中包括学术论文网站pubmud，arxiv，代码共享平台github，编程交流平台stack exchange，预处理图书数据集BookCorpus 2、Books3、OpenWebText2，以及其他相关高质量数据集等。悟道<sup id="fnref:6"><a class="footnote-ref" href="#fn:6" role="doc-noteref">6</a></sup>同样的采用了大量的网络语料，与前面不同的是，其主要筛选中文语料，并对中文字符少于10个的网页进行了丢弃。</p>
<p>另外，经过我们的调查发现，Pile<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" role="doc-noteref">5</a></sup>作为处理质量较高的数据集在开源后被广泛使用，后续的一系列工作<sup id="fnref:7"><a class="footnote-ref" href="#fn:7" role="doc-noteref">7</a></sup><sup id="fnref:8"><a class="footnote-ref" href="#fn:8" role="doc-noteref">8</a></sup><sup id="fnref:9"><a class="footnote-ref" href="#fn:9" role="doc-noteref">9</a></sup>倾向于在与训练语料中加入Pile或者Pile CC，以节省数据处理的时间，同时加入部分更新时间戳下的Common Crwal语料，以保证模型的质量与时效性。</p>
<p><img alt="数据来源汇总_page-0001" src="https://s2.loli.net/2023/03/06/pbYFACMl8divjo5.jpg"/></p>
</li>
<li>
<p>数据处理</p>
<p>数据处理方面主要存在两个分解的问题：数据处理方法以及数据处理流程，之前的工作说明了其对于大规模预训练语料的处理流程，整体都可以分为三个模块，对于网络信息的抽取与转换，对于数据的评估与清洗，对于数据的去重。</p>
<p>由于网页中的信息一般采用html语言并经过浏览器的解析被人们所看到，也就是说，一般来说，被我们直接爬取的信息以html的形式保存在原始数据汇总（在Common crawl中以warc的形式存储在数据库中），html语言对于人类语言来说存在非常多用于排版的tag从而带来较多的信息冗余。基于这个原因，一部分工作采用wet格式【一种common crawl 从网页中抽取文本组成的格式】来进行进一步的预处理，例如GPT-3<sup id="fnref:4"><a class="footnote-ref" href="#fn:4" role="doc-noteref">4</a></sup>的原始版本训练时采用WET格式，后续对其进行了文档级别的筛选，T5<sup id="fnref:10"><a class="footnote-ref" href="#fn:10" role="doc-noteref">10</a></sup>采用WET格式预处理的方式比较严格，其采用了一系列启发性的操作，例如关键字匹配，关键语法匹配等。CC Net 同样采用WET格式进行文档级别的筛选，但是该类问题的处理方式丢失了数据的格式，仅仅保留了文本。因此，另外一部分工作采用WARC格式来进行下一部分的与处理。Pile<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" role="doc-noteref">5</a></sup>认为，采用WET格式数据将会造成文档内模版数据无法删除，即便采用去重算法以及关键字匹配算法，处理后的数据质量仍然较低。其实验了几种提取工具，由于其准确丢弃率较高，因此采用jusText<sup id="fnref:11"><a class="footnote-ref" href="#fn:11" role="doc-noteref">11</a></sup>用以处理WARC格式内部的模版文件，并提取网页模版内部的文本数据。MT-NLG<sup id="fnref:7"><a class="footnote-ref" href="#fn:7" role="doc-noteref">7</a></sup> 也延续了这一操作。</p>
<p>对于特殊高质量语料的转换处理，该部分存在几种情况，一种是挑选出的高质量网站爬取的数据，由于该部分网站模式单一，可以采用固定的几种提取方式将正文直接提取出来，对于非html格式组织的数据，例如pile<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" role="doc-noteref">5</a></sup>中所引入的arxiv的Tex 格式与Pubmed Central中按照JATS格式组织的论文将通过pandoc<sup id="fnref:12"><a class="footnote-ref" href="#fn:12" role="doc-noteref">12</a></sup>进行处理为markdown形式，该格式保留了文本的原始结构特征，其数据的表达能力有所提升的同时防止大量无关标签的引入导致数据的质量下降。类似于zhihu或者 Stack Exchange网站中的带有明显「问题，答案」结构的数据可以对其进行结构化处理，例如加入Q：\n\n A:\n\n来将其转化为带有QA结构的数据。对于已经预处理好的数据，或者存在开源版本特定算法的数据，例如wiki ，大部分工作认为可以直接将其引入到预训练语料当中，这一部分数据其质量是最高的，所蕴含的知识相对于其他语料也较多，因此，该部分语料在训练过程中通常需要上采样，以增加模型学习高质量语料的比例。</p>
</li>
<li>
<p>数据清洗</p>
<ol>
<li>
<p>通用数据筛选</p>
</li>
<li>
<p>数据质量筛选</p>
</li>
<li>
<p>数据安全性筛选</p>
</li>
<li>
<p>数据重复性筛选</p>
</li>
</ol>
</li>
<li>
<p>数据质量评价</p>
<p>对预处理数据的评价标准是十分难以鉴别的，在没有训练，测评之前，我们并不知道一个真正优质的，鲁棒的，无偏差的，高知识的模型需要何种数据。而理论上高质量且多样的数据将会训练出一个更好的模型。目前的大规模语言模型通常情况下采用T级别的语料进行训练，LLama<sup id="fnref:13"><a class="footnote-ref" href="#fn:13" role="doc-noteref">13</a></sup>等工作表明，更多的数据有利于模型对于语言的学习。T5，FLAN等工作表示引入部分下游任务有利于模型性能的提升，而很多的已知的工作表示，更多的来源以及更精细化的处理可能会给语言模型带来更好的性能。因此，对于前期数据质量的评定在一定程度上也可以反映出训练后模型的性能。当前，研究人员主要集中在数据的多样性与基于模型的数据质量对于预训练的数据进行评价。</p>
<p>对于数据的多样性，其主要表现为数据来源的多样，语言的多样，主题的多样以及表达方式的多样， 大部分工作都选择采用多来源的数据集进行训练，一方面，多来源的数据可以收集尽可能多的数据，补充数据的数量，另一方面，多个来源的数据往往会有不同的主题或者表达方式，往往其多样性也可以得到保证。为了衡量或者评估数据集的多样性，部分工作采用topic model的方式，希望判断数据集主题的多样性来对数据集进行分析，PaLM<sup id="fnref:14"><a class="footnote-ref" href="#fn:14" role="doc-noteref">14</a></sup>采用Hierarchical topic modeling的方式，对数据集进行评估，Pile在Pilecc中训练了一个16gram的topic model 作为基准，以此衡量各个子数据集的多样性。</p>
<p><img alt="PaLM的多层主题检测" src="https://s2.loli.net/2023/03/06/e6vYmWtpOybqfZN.png"/></p>
<p><img alt="pile的数据集多样性验证" src="https://s2.loli.net/2023/03/06/ZAbBk6iXfTtWj7s.png"/></p>
<p>对于数据集的质量，目前并没有较好的解决方案。大部分的工作采用另一个语言模型来计算在该数据集上的困惑度。例如，pile在其论文中针对GPT-2与GPT-3对pile数据集的困惑度，后续的部分工作为了证明模型的效果，同样的在pile数据集中验证困惑度，一方面通过GPT-3证明其数据具有较为优质的质量【符合模型生成或者训练的需要】，另一方面模型通过数据也可以验证其效果，但是该方法存在弊端，即无法证明数据与模型是否最优以及如何达到最优。另外，根据【引用】所实验，困惑度相对于中文可能并不是一个很优质的指标，需要寻找另外的一个指标来对中文的质量进行分类。</p>
<p>另外，pile<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" role="doc-noteref">5</a></sup>同样的对于各个子数据进行了13-gram的同一性检测，大部分的13-gram重复为格式文件，html符号，网站的评论或者正文模版信息。这些信息不仅仅对于数据质量存在影响，可能需要后续工作提出更好的数据预处理方案。</p>
<p>另外，更多的验证包含数据的偏见，毒性等可以采用采用计算协同性质的方法来进行评估<sup id="fnref:14"><a class="footnote-ref" href="#fn:14" role="doc-noteref">14</a></sup>。</p>
</li>
</ol>
<h2 id="refenrence">Refenrence</h2>
<section class="footnotes" role="doc-endnotes">
<hr/>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://www.easytechjunkie.com/how-big-is-the-internet.htm#:~:text=That's%20over%205%20billion%20gigabytes%20of%20data%2C%20or%205%20trillion%20megabytes">https://www.easytechjunkie.com/how-big-is-the-internet.htm#:~:text=That's%20over%205%20billion%20gigabytes%20of%20data%2C%20or%205%20trillion%20megabytes</a>. <a class="footnote-backref" href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://commoncrawl.org/">https://commoncrawl.org/</a> <a class="footnote-backref" href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>A. Askell等, 《A General Language Assistant as a Laboratory for Alignment》. arXiv, 2021年12月9日. 见于: 2023年3月6日. [在线]. 载于: <a href="http://arxiv.org/abs/2112.00861">http://arxiv.org/abs/2112.00861</a> <a class="footnote-backref" href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>T. B. Brown等, 《Language Models are Few-Shot Learners》. arXiv, 2020年7月22日. [在线]. 载于: <a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a> <a class="footnote-backref" href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>L. Gao等, 《The Pile: An 800GB Dataset of Diverse Text for Language Modeling》. arXiv, 2020年12月31日. [在线]. 载于: <a href="http://arxiv.org/abs/2101.00027">http://arxiv.org/abs/2101.00027</a> <a class="footnote-backref" href="#fnref:5" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>《WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models - ScienceDirect》. <a href="https://www.sciencedirect.com/science/article/pii/S2666651021000152?ref=pdf_download&amp;fr=RR-2&amp;rr=7a29c845ab86f963">https://www.sciencedirect.com/science/article/pii/S2666651021000152?ref=pdf_download&amp;fr=RR-2&amp;rr=7a29c845ab86f963</a> <a class="footnote-backref" href="#fnref:6" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>S. Smith等, 《Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model》. arXiv, 2022年2月4日. 见于: 2023年3月4日. [在线]. 载于: <a href="http://arxiv.org/abs/2201.11990">http://arxiv.org/abs/2201.11990</a> <a class="footnote-backref" href="#fnref:7" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>S. Black等, 《GPT-NeoX-20B: An Open-Source Autoregressive Language Model》. arXiv, 2022年4月14日. 见于: 2023年3月4日. [在线]. 载于: <a href="http://arxiv.org/abs/2204.06745">http://arxiv.org/abs/2204.06745</a> <a class="footnote-backref" href="#fnref:8" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>S. Zhang等, 《OPT: Open Pre-trained Transformer Language Models》. arXiv, 2022年6月21日. 见于: 2023年3月4日. [在线]. 载于: <a href="http://arxiv.org/abs/2205.01068">http://arxiv.org/abs/2205.01068</a> <a class="footnote-backref" href="#fnref:9" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>C. Raffel等, 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》. arXiv, 2020年7月28日. 见于: 2023年3月6日. [在线]. 载于: <a href="http://arxiv.org/abs/1910.10683%5D">http://arxiv.org/abs/1910.10683]</a> <a class="footnote-backref" href="#fnref:10" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>I. Endrédy和A. Novák, 《More Effective Boilerplate Removal-the GoldMiner Algorithm》, <em>Polibits</em>, 期 48, 页 79–83, 12月 2013. <a class="footnote-backref" href="#fnref:11" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p><a href="https://pandoc.org/">https://pandoc.org/</a> <a class="footnote-backref" href="#fnref:12" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p>R. Thoppilan等, 《LaMDA: Language Models for Dialog Applications》. arXiv, 2022年2月10日. 见于: 2023年3月6日. [在线]. 载于: <a href="http://arxiv.org/abs/2201.08239">http://arxiv.org/abs/2201.08239</a> <a class="footnote-backref" href="#fnref:13" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p>A. Chowdhery等, 《PaLM: Scaling Language Modeling with Pathways》. arXiv, 2022年10月5日. 见于: 2023年3月6日. [在线]. 载于: <a href="http://arxiv.org/abs/2204.02311">http://arxiv.org/abs/2204.02311</a> <a class="footnote-backref" href="#fnref:14" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
</section>
</article>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ngc7293s-blog'; 

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://ngc7293s-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<footer id="footer">
<div id="social">
<a class="symbol" href="https://www.facebook.com/">
<i class="fa fa-facebook-square"></i>
</a>
<a class="symbol" href="https://www.github.com/ngc7292">
<i class="fa fa-github-square"></i>
</a>
<a class="symbol" href="https://www.twitter.com/">
<i class="fa fa-twitter-square"></i>
</a>
</div>
<p class="small">
    
       © Copyright 2023 <i aria-hidden="true" class="fa fa-heart"></i> ngc7293
    
    </p>
<p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
</p>
</footer>
</section>
<script src="https://ngc7292.github.io/js/jquery-2.2.4.min.js"></script>
<script src="https://ngc7292.github.io/js/main.js"></script>
<script src="https://ngc7292.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'ngc7293', 'auto');
	
	ga('send', 'pageview');
}
</script>
</body>
</html>
