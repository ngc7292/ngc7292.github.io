<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		 
			
  
    <meta name="twitter:card" content="summary"/>
    
      <meta name="twitter:image" content="https://ngc7292.github.io/images/avatar.png" />
    
  
  
  <meta name="twitter:title" content="neural network and BP algorithm"/>
  <meta name="twitter:description" content="learning by ngc7293"/>
  
    <meta name="twitter:site" content="@ngc7293"/>
  
  
  
  
    <meta name="twitter:creator" content="@ngc7293"/>
  



		
		<meta name="author" content="ngc7293">
		
		<meta name="generator" content="Hugo 0.36.1" />
		<title>neural network and BP algorithm &middot; ngc7293&#39;s blog</title>
		<link rel="shortcut icon" href="https://ngc7292.github.io/images/favicon.ico">
		<link rel="stylesheet" href="https://ngc7292.github.io/css/style.css">
		<link rel="stylesheet" href="https://ngc7292.github.io/css/highlight.css">

		
		<link rel="stylesheet" href="https://ngc7292.github.io/css/font-awesome.min.css">
		

		
		<link href="https://ngc7292.github.io/index.xml" rel="alternate" type="application/rss+xml" title="ngc7293&#39;s blog" />
		

		

		<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://ngc7292.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://ngc7292.github.io/posts'>Archive</a>
	<a href='https://ngc7292.github.io/tags'>Tags</a>
	<a href='https://ngc7292.github.io/about'>About</a>

	

	
	<a class="cta" href="https://ngc7292.github.io/index.xml">Subscribe</a>
	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        neural network and BP algorithm
                    </h1>
                    <h2 class="headline">
                    Jul 27, 2018 00:00
                    · 388 words
                    · 2 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://ngc7292.github.io/tags/mechine-learning">mechine learning</a>
                          
                              <a href="https://ngc7292.github.io/tags/algorithm">algorithm</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                  
                    <div id="toc">
                      <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#神经网络简述">神经网络简述</a></li>
<li><a href="#感知机与多重神经网络">感知机与多重神经网络</a></li>
<li><a href="#bp-误差逆传播算法">BP(误差逆传播算法)</a>
<ul>
<li>
<ul>
<li><a href="#标准bp算法和累积bp算法">标准BP算法和累积BP算法</a></li>
<li><a href="#过拟合的解决">过拟合的解决</a></li>
</ul></li>
</ul></li>
<li><a href="#迭代优化算法">迭代优化算法</a>
<ul>
<li>
<ul>
<li><a href="#梯度下降法">梯度下降法</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
                    </div>
                  
                
                <section id="post-body">
                    <p>西瓜书的笔记</p>

<p></p>

<h2 id="神经网络简述">神经网络简述</h2>

<p>神经网络最基本组成成分是神经元，每个神经元输入的信号通过带权连接进入相应的神经元，并将总输入值与阈值进行比较，并通过激活函数进行输出。</p>

<p>典型的激活函数包括像sigmoid函数或者跃迁函数之类的，sigmoid函数又被称为挤压函数，好像是说把数字都挤压到[0,1]区间上的函数，其数学表示为：</p>

<p>$$ sigmoid(x) = 1/(1+e^{-x}) $$</p>

<p>函数图形类似这样：</p>

<p><img src="http://ngcimage.tan90.me/image/blog/sigmoid.jpg" alt="" /></p>

<p>很多神经元连接于是得到神经网络，从计算机科学的角度，神经网络可以看作一个模型，该模型有许多函数，并且在很多时候可以递归什么的，最后给出一个输出，类似的，实现下列相互嵌套，相互作用的一些函数综合而成的模型，很多都是以数学证明为基础的。</p>

<p>$$ y_j = f(\Sigma_i w_ix_i - \theta_j)$$</p>

<h2 id="感知机与多重神经网络">感知机与多重神经网络</h2>

<p>感知机是我感觉最简单的一种神经网络，其由两层神经元构成的，对于线性可分问题比如关于与，或，非等问题，即存在一个线性超平面可以将所有的输入划分为两个模式那种，对于非线性可分问题，感知机可能会有些问题，比如产生震荡（fluctuation），即一直在两个模块之间跌宕，得不到最后的x的参数。</p>

<p>当出现这了问题时，我们需要多层神经网络，一般常见的有多层前馈网络，由输入层处理外界信号，由隐层处理数据，由输出层输出，网络的学习过程就是对其中的连接权，以及各个层之间的阈值。</p>

<p><img src="http://ngcimage.tan90.me/image/blog/mLP.jpg" alt="" /></p>

<h2 id="bp-误差逆传播算法">BP(误差逆传播算法)</h2>

<p>首先给定一个训练集</p>

<p>$$ D = {(x_1,y_1),(x_2,y_2),&hellip;,(x_m,y_m)} x_i \in R^d y_i \in R^l$$</p>

<p>表示训练集由d个属性描述，输出l维实值向量</p>

<p>下面是该算法的一些表示：</p>

<p>$$ \theta_j 表示输出层第j个神经元阈值$$</p>

<p>$$ \gamma_{h} 表示隐层第h个神经元的阈值$$</p>

<p>$$ \nu_{ih} 表示第i个输入层神经元到第h个隐层神经元的连接权$$</p>

<p>$$ \omega_{hj} 表示第h个隐层神经元到第j个输出层神经元的连接权$$</p>

<p>$$ \alpha_{h} = \sum ^{d} _{i=1}\nu _{ih}x_i 表示隐层神经元接收到的输入$$</p>

<p>$$ \beta_j = \sum^{q} _{h=1} \omega _{hi}b_h 表示输出层第j个神经元接收的输入$$</p>

<p>假设神经元都使用sigmoid函数，假定神经网络输出为：</p>

<p>$$ \widehat {y}^k_j = f(\beta_j - \theta_j) $$</p>

<p>均方误差：</p>

<p>$$ E_{k}=\dfrac {1}{2}\sum ^{l} _{j=1}( \widehat {y}^k _{j}-y^k _{j}) $$</p>

<p>注：1/2的是为了求导方便</p>

<p>BP算法是基于梯度下降策略的，以目标的负梯度方向对参数进行调整。</p>

<p>$$\Delta \omega _{hj}=-\eta\dfrac {\partial E _{k}}{\partial \omega _{hj}}​$$</p>

<p>$$\Delta \theta _{j}=-\eta\dfrac {\partial E _{k}}{\partial \beta _{j}}​$$</p>

<p>$$\Delta \nu _{ih}=-\eta\dfrac {\partial E _{k}}{\partial \nu _{ih}}$$</p>

<p>以$\omega$举例，$\omega$是先影响到$\beta$再影响y继而影响E的，所以我们可以将$\omega$式子转化：</p>

<p>$$\dfrac {\partial E _{k}} {\partial \omega _{hj}} = \dfrac {\partial E _{k}} {\partial y^k _{j}}* \dfrac {\partial y^k _{j}} {\partial \beta _{j}}* \dfrac {\partial \beta _{j}} {\partial \omega _{hj}}$$</p>

<p>由于</p>

<p>$$f&rsquo;\left( x\right) = f(x)(1-f(x))$$</p>

<p>设</p>

<p>$$g_j = -\dfrac {\partial E _{k}} {\partial y^k _{j}}*\dfrac {\partial y^k _{j}} {\partial \beta _{j}} = \widehat{y}^k _j(1-\widehat{y}^k _j)(y^k _j-\widehat{y}^k _j)$$</p>

<p>所以关于omega的更新公式为</p>

<p>$$\Delta \omega_{hj}=\eta g_jb_n$$</p>

<p>其他的更新公式为：</p>

<p>$$ \Delta \theta_{j}=-\eta g_j $$</p>

<p>$$ \Delta \nu_{ih}=\eta e_h x_i $$</p>

<p>$$ \Delta \gamma_{h}=-\eta e_h $$</p>

<p>之后便是遍历训练集并更新其中的一个参数</p>

<p>另外，BP算法的目标是最小化训练集上的累积误差:</p>

<p>$$ E = 1/m\sum^m_{k=1}E_k ​$$</p>

<p>为了使累计误差达到最小，我们通常采用更多次数的迭代。</p>

<h4 id="标准bp算法和累积bp算法">标准BP算法和累积BP算法</h4>

<p>标准BP算法是基于单个$E_k$推导出来的，类似的，基于累积误差最小化的更新规则，就得到累积BP算法。</p>

<p>由于标准BP算法针对单个样例，更新迭代较慢，而累积BP采用累积误差，每次迭代可以针对多个样例，更新参数的频率就会低很多。</p>

<h4 id="过拟合的解决">过拟合的解决</h4>

<p>过拟合是指其训练误差持续降低，但是测试误差却不断升高的现象，主要原因是因为特征太多，模型比较复杂，参数较多，训练数据较少，噪声过多，过度拟合了训练数据但是并没有考虑到泛化能力。</p>

<p>减少过拟合的方式书上给了两种，一种策略是早停，设置误差阈值，训练级以及测试集，若训练误差降低但是测试误差升高，返回较小的连接权。</p>

<p>另一种是正则化 基本思想就是在误差目标函数中增加一个描述网络复杂度的部分，例如连接权和阈值的平方。</p>

<h2 id="迭代优化算法">迭代优化算法</h2>

<h4 id="梯度下降法">梯度下降法</h4>

<p>参考文章：<a href="https://www.jianshu.com/p/c7e642877b0e">深入浅出&ndash;梯度下降法及其实现</a></p>

<p>梯度下降法是最小二乘法的一种，用于求解无约束规划问题，用于获得损失函数或者代价函数的最小化。</p>

<p>定义一个函数$$ C = f(v_1,&hellip;,v_m)$$</p>

<p>梯度的定义为 $$∇C≡(∂C/∂v_1,…,∂C/∂v_m)^{T}$$</p>

<p>更新梯度$$ v \rightarrow  v&rsquo; = v - \eta \nabla C$$</p>

<p>直到v到无穷小的时候，我们可以最小化C。</p>

<p>梯度下降算法有大致三种形式，BGD（批量梯度下降），SGD（随机梯度下降），MBGD（小批量随机梯度下降）</p>

<p>例如我们对一组数据进行线性回归拟合，其线性函数为</p>

<p>$$ y = b+w*x$$</p>

<p>$w$ 作为权重，$b$作为偏置，我们定义一个loss函数为$loss = 1/2m\sum_n(w*x-y)^2$</p>

<p>我们将loss函数改变一个写法 $loss = 1/2m\sum_n(W*X-Y)^{T}(W*X-Y)$，便于计算，计算loss函数的微分：$loss = 1/m \sum_n (W*X-Y)X^{T}$</p>

<p>通过数据计算loss函数的值，不断逼近loss函数的最小化</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
m = 20

X0 = np.ones((m, 1))
X1 = np.arange(1, m + 1).reshape(m, 1)

X = np.hstack((X0, X1))

Y = np.array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m, 1)

lr = 0.01


def error_function(theta, x, y):
    diff = np.dot(x, theta) - y
    return (1 / 2. * m) * (np.dot(np.transpose(diff), diff))


def gradient_function(theta, x, y):
    diff = np.dot(x, theta) - y
    return (1 / m) * (np.dot(np.transpose(x), diff))

def gradient_descent(x,y,lr):
    theta = np.array([1,1]).reshape(2,1)
    index = 0
    gradient = gradient_function(theta,x,y)
    while not np.all(np.absolute(gradient) &lt;= 1e-5):
        index += 1
        theta = theta - lr*gradient
        gradient = gradient_function(theta,x,y)
    print(&quot;gradient descent finish &quot;+str(index)+&quot; traing&quot;)
    return theta
</code></pre>

<p>上述BGD虽然对所有样本进行计算，可以很好的得到全局最优，但是性能并不是很好，由于每次迭代都要计算所有样本，训练过程会很慢。</p>

<p>SGD，每轮迭代中都只随机优化某一条训练数据的损失，更新速度比较快，但是会更新至局部最优。</p>

<p>repeat{
    for i=1,&hellip;,m{
       𝜃𝑗:=𝜃𝑗−𝛼(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))𝑥(𝑖)𝑗θj:=θj−α(hθ(x(i))−y(i))xj(i)
      (for j =0,1)
    }
}</p>

<p>MBGD，随机计算小批次的数据，是MGD以及SGD的折中版本。</p>

<p>repeat{
    for i=1,11,21,31,&hellip;,991{
       𝜃𝑗:=𝜃𝑗−𝛼110∑(𝑖+9)𝑘=𝑖(ℎ𝜃(𝑥(𝑘))−𝑦(𝑘))𝑥(𝑘)𝑗θj:=θj−α110∑k=i(i+9)(hθ(x(k))−y(k))xj(k)
      (for j =0,1)
    }
  }</p>
                </section>
            </article>

            
                <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fngc7292.github.io%2fposts%2fneural_networks%2f - neural_network_and_BP_algorithm by @ngc7293"><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ngc7293s-blog'; 

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://ngc7293s-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.facebook.com/">
        <i class="fa fa-facebook-square"></i>
    </a>
    
    <a class="symbol" href="https://www.github.com/ngc7292">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.twitter.com/">
        <i class="fa fa-twitter-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2020 <i class="fa fa-heart" aria-hidden="true"></i> ngc7293
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="https://ngc7292.github.io/js/jquery-2.2.4.min.js"></script>
<script src="https://ngc7292.github.io/js/main.js"></script>
<script src="https://ngc7292.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'ngc7293', 'auto');
ga('send', 'pageview');
</script>





    </body>
</html>
