<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="summary" name="twitter:card"/>
<meta content="https://ngc7292.github.io/images/avatar.png" name="twitter:image"/>
<meta content="neural network and BP algorithm" name="twitter:title"/>
<meta content="learning by ngc7293" name="twitter:description"/>
<meta content="@ngc7293" name="twitter:site"/>
<meta content="@ngc7293" name="twitter:creator"/>
<meta content="ngc7293" name="author"/>
<meta content="Hugo 0.78.1" name="generator"/>
<title>neural network and BP algorithm · ngc7293's blog</title>
<link href="https://ngc7292.github.io/images/favicon.ico" rel="shortcut icon"/>
<link href="https://ngc7292.github.io/css/style.css" rel="stylesheet"/>
<link href="https://ngc7292.github.io/css/highlight.css" rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/gh/ngc7292/live2d-widget@latest/autoload.js"></script>
<link href="https://ngc7292.github.io/css/font-awesome.min.css" rel="stylesheet"/>
<link href="https://ngc7292.github.io/index.xml" rel="alternate" title="ngc7293's blog" type="application/rss+xml"/>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
</head>
<body>
<nav class="main-nav">
<a href="https://ngc7292.github.io/"> <span class="arrow">←</span>Home</a>
<a href="https://ngc7292.github.io/posts">Archive</a>
<a href="https://ngc7292.github.io/tags">Tags</a>
<a href="https://ngc7292.github.io/about">About</a>
<a class="cta" href="https://ngc7292.github.io/index.xml">Subscribe</a>
</nav>
<section class="post" id="wrapper">
<article>
<header>
<h1>
                        neural network and BP algorithm
                    </h1>
<h2 class="headline">
                    Jan 1, 1970 08:33
                    · 388 words
                    · 2 minute read
                      <span class="tags">
<a href="https://ngc7292.github.io/tags/mechine-learning">mechine learning</a>
<a href="https://ngc7292.github.io/tags/algorithm">algorithm</a>
</span>
</h2>
</header>
<div id="toc">
<nav id="TableOfContents">
<ul>
<li><a href="#神经网络简述">神经网络简述</a></li>
<li><a href="#感知机与多重神经网络">感知机与多重神经网络</a></li>
<li><a href="#bp误差逆传播算法">BP(误差逆传播算法)</a>
<ul>
<li></li>
</ul>
</li>
<li><a href="#迭代优化算法">迭代优化算法</a>
<ul>
<li></li>
</ul>
</li>
</ul>
</nav>
</div>
<section id="post-body">
<p>西瓜书的笔记</p>
<h2 id="神经网络简述">神经网络简述</h2>
<p>神经网络最基本组成成分是神经元，每个神经元输入的信号通过带权连接进入相应的神经元，并将总输入值与阈值进行比较，并通过激活函数进行输出。</p>
<p>典型的激活函数包括像sigmoid函数或者跃迁函数之类的，sigmoid函数又被称为挤压函数，好像是说把数字都挤压到[0,1]区间上的函数，其数学表示为：</p>
<p>$$ sigmoid(x) = 1/(1+e^{-x}) $$</p>
<p>函数图形类似这样：</p>
<p><img alt="" src="http://ngcimage.tan90.me/image/blog/sigmoid.jpg"/></p>
<p>很多神经元连接于是得到神经网络，从计算机科学的角度，神经网络可以看作一个模型，该模型有许多函数，并且在很多时候可以递归什么的，最后给出一个输出，类似的，实现下列相互嵌套，相互作用的一些函数综合而成的模型，很多都是以数学证明为基础的。</p>
<p>$$ y_j = f(\Sigma_i w_ix_i - \theta_j)$$</p>
<h2 id="感知机与多重神经网络">感知机与多重神经网络</h2>
<p>感知机是我感觉最简单的一种神经网络，其由两层神经元构成的，对于线性可分问题比如关于与，或，非等问题，即存在一个线性超平面可以将所有的输入划分为两个模式那种，对于非线性可分问题，感知机可能会有些问题，比如产生震荡（fluctuation），即一直在两个模块之间跌宕，得不到最后的x的参数。</p>
<p>当出现这了问题时，我们需要多层神经网络，一般常见的有多层前馈网络，由输入层处理外界信号，由隐层处理数据，由输出层输出，网络的学习过程就是对其中的连接权，以及各个层之间的阈值。</p>
<p><img alt="" src="http://ngcimage.tan90.me/image/blog/mLP.jpg"/></p>
<h2 id="bp误差逆传播算法">BP(误差逆传播算法)</h2>
<p>首先给定一个训练集</p>
<p>$$ D = {(x_1,y_1),(x_2,y_2),…,(x_m,y_m)} x_i \in R^d y_i \in R^l$$</p>
<p>表示训练集由d个属性描述，输出l维实值向量</p>
<p>下面是该算法的一些表示：</p>
<p>$$ \theta_j 表示输出层第j个神经元阈值$$</p>
<p>$$ \gamma_{h} 表示隐层第h个神经元的阈值$$</p>
<p>$$ \nu_{ih} 表示第i个输入层神经元到第h个隐层神经元的连接权$$</p>
<p>$$ \omega_{hj} 表示第h个隐层神经元到第j个输出层神经元的连接权$$</p>
<p>$$ \alpha_{h} = \sum ^{d} _{i=1}\nu _{ih}x_i 表示隐层神经元接收到的输入$$</p>
<p>$$ \beta_j = \sum^{q} _{h=1} \omega _{hi}b_h 表示输出层第j个神经元接收的输入$$</p>
<p>假设神经元都使用sigmoid函数，假定神经网络输出为：</p>
<p>$$ \widehat {y}^k_j = f(\beta_j - \theta_j) $$</p>
<p>均方误差：</p>
<p>$$ E_{k}=\dfrac {1}{2}\sum ^{l} _{j=1}( \widehat {y}^k _{j}-y^k _{j}) $$</p>
<p>注：1/2的是为了求导方便</p>
<p>BP算法是基于梯度下降策略的，以目标的负梯度方向对参数进行调整。</p>
<p>$$\Delta \omega _{hj}=-\eta\dfrac {\partial E _{k}}{\partial \omega _{hj}}​$$</p>
<p>$$\Delta \theta _{j}=-\eta\dfrac {\partial E _{k}}{\partial \beta _{j}}​$$</p>
<p>$$\Delta \nu _{ih}=-\eta\dfrac {\partial E _{k}}{\partial \nu _{ih}}$$</p>
<p>以$\omega$举例，$\omega$是先影响到$\beta$再影响y继而影响E的，所以我们可以将$\omega$式子转化：</p>
<p>$$\dfrac {\partial E _{k}} {\partial \omega _{hj}} = \dfrac {\partial E _{k}} {\partial y^k _{j}}* \dfrac {\partial y^k _{j}} {\partial \beta _{j}}* \dfrac {\partial \beta _{j}} {\partial \omega _{hj}}$$</p>
<p>由于</p>
<p>$$f'\left( x\right) = f(x)(1-f(x))$$</p>
<p>设</p>
<p>$$g_j = -\dfrac {\partial E _{k}} {\partial y^k _{j}}*\dfrac {\partial y^k _{j}} {\partial \beta _{j}} = \widehat{y}^k _j(1-\widehat{y}^k _j)(y^k _j-\widehat{y}^k _j)$$</p>
<p>所以关于omega的更新公式为</p>
<p>$$\Delta \omega_{hj}=\eta g_jb_n$$</p>
<p>其他的更新公式为：</p>
<p>$$ \Delta \theta_{j}=-\eta g_j $$</p>
<p>$$ \Delta \nu_{ih}=\eta e_h x_i $$</p>
<p>$$ \Delta \gamma_{h}=-\eta e_h $$</p>
<p>之后便是遍历训练集并更新其中的一个参数</p>
<p>另外，BP算法的目标是最小化训练集上的累积误差:</p>
<p>$$ E = 1/m\sum^m_{k=1}E_k ​$$</p>
<p>为了使累计误差达到最小，我们通常采用更多次数的迭代。</p>
<h4 id="标准bp算法和累积bp算法">标准BP算法和累积BP算法</h4>
<p>标准BP算法是基于单个$E_k$推导出来的，类似的，基于累积误差最小化的更新规则，就得到累积BP算法。</p>
<p>由于标准BP算法针对单个样例，更新迭代较慢，而累积BP采用累积误差，每次迭代可以针对多个样例，更新参数的频率就会低很多。</p>
<h4 id="过拟合的解决">过拟合的解决</h4>
<p>过拟合是指其训练误差持续降低，但是测试误差却不断升高的现象，主要原因是因为特征太多，模型比较复杂，参数较多，训练数据较少，噪声过多，过度拟合了训练数据但是并没有考虑到泛化能力。</p>
<p>减少过拟合的方式书上给了两种，一种策略是早停，设置误差阈值，训练级以及测试集，若训练误差降低但是测试误差升高，返回较小的连接权。</p>
<p>另一种是正则化 基本思想就是在误差目标函数中增加一个描述网络复杂度的部分，例如连接权和阈值的平方。</p>
<h2 id="迭代优化算法">迭代优化算法</h2>
<h4 id="梯度下降法">梯度下降法</h4>
<p>参考文章：<a href="https://www.jianshu.com/p/c7e642877b0e">深入浅出–梯度下降法及其实现</a></p>
<p>梯度下降法是最小二乘法的一种，用于求解无约束规划问题，用于获得损失函数或者代价函数的最小化。</p>
<p>定义一个函数$$ C = f(v_1,…,v_m)$$</p>
<p>梯度的定义为 $$∇C≡(∂C/∂v_1,…,∂C/∂v_m)^{T}$$</p>
<p>更新梯度$$ v \rightarrow  v' = v - \eta \nabla C$$</p>
<p>直到v到无穷小的时候，我们可以最小化C。</p>
<p>梯度下降算法有大致三种形式，BGD（批量梯度下降），SGD（随机梯度下降），MBGD（小批量随机梯度下降）</p>
<p>例如我们对一组数据进行线性回归拟合，其线性函数为</p>
<p>$$ y = b+w*x$$</p>
<p>$w$ 作为权重，$b$作为偏置，我们定义一个loss函数为$loss = 1/2m\sum_n(w*x-y)^2$</p>
<p>我们将loss函数改变一个写法 $loss = 1/2m\sum_n(W<em>X-Y)^{T}(W</em>X-Y)$，便于计算，计算loss函数的微分：$loss = 1/m \sum_n (W*X-Y)X^{T}$</p>
<p>通过数据计算loss函数的值，不断逼近loss函数的最小化</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
m <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>

X0 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones((m, <span style="color:#ae81ff">1</span>))
X1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, m <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>reshape(m, <span style="color:#ae81ff">1</span>)

X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>hstack((X0, X1))

Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
    <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">12</span>,
    <span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">18</span>, <span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">19</span>, <span style="color:#ae81ff">21</span>
])<span style="color:#f92672">.</span>reshape(m, <span style="color:#ae81ff">1</span>)

lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">error_function</span>(theta, x, y):
    diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x, theta) <span style="color:#f92672">-</span> y
    <span style="color:#66d9ef">return</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.</span> <span style="color:#f92672">*</span> m) <span style="color:#f92672">*</span> (np<span style="color:#f92672">.</span>dot(np<span style="color:#f92672">.</span>transpose(diff), diff))


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient_function</span>(theta, x, y):
    diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x, theta) <span style="color:#f92672">-</span> y
    <span style="color:#66d9ef">return</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> m) <span style="color:#f92672">*</span> (np<span style="color:#f92672">.</span>dot(np<span style="color:#f92672">.</span>transpose(x), diff))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient_descent</span>(x,y,lr):
    theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>])<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)
    index <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    gradient <span style="color:#f92672">=</span> gradient_function(theta,x,y)
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> np<span style="color:#f92672">.</span>all(np<span style="color:#f92672">.</span>absolute(gradient) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1e-5</span>):
        index <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        theta <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> lr<span style="color:#f92672">*</span>gradient
        gradient <span style="color:#f92672">=</span> gradient_function(theta,x,y)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">"gradient descent finish "</span><span style="color:#f92672">+</span>str(index)<span style="color:#f92672">+</span><span style="color:#e6db74">" traing"</span>)
    <span style="color:#66d9ef">return</span> theta
</code></pre></div><p>上述BGD虽然对所有样本进行计算，可以很好的得到全局最优，但是性能并不是很好，由于每次迭代都要计算所有样本，训练过程会很慢。</p>
<p>SGD，每轮迭代中都只随机优化某一条训练数据的损失，更新速度比较快，但是会更新至局部最优。</p>
<pre><code>repeat{
    for i=1,...,m{
       𝜃𝑗:=𝜃𝑗−𝛼(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))𝑥(𝑖)𝑗θj:=θj−α(hθ(x(i))−y(i))xj(i)
      (for j =0,1)
    }
}
</code></pre><p>MBGD，随机计算小批次的数据，是MGD以及SGD的折中版本。</p>
<pre><code>repeat{
    for i=1,11,21,31,...,991{
       𝜃𝑗:=𝜃𝑗−𝛼110∑(𝑖+9)𝑘=𝑖(ℎ𝜃(𝑥(𝑘))−𝑦(𝑘))𝑥(𝑘)𝑗θj:=θj−α110∑k=i(i+9)(hθ(x(k))−y(k))xj(k)
      (for j =0,1)
    }
  }
</code></pre>
</section>
</article>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ngc7293s-blog'; 

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://ngc7293s-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<footer id="footer">
<div id="social">
<a class="symbol" href="https://www.facebook.com/">
<i class="fa fa-facebook-square"></i>
</a>
<a class="symbol" href="https://www.github.com/ngc7292">
<i class="fa fa-github-square"></i>
</a>
<a class="symbol" href="https://www.twitter.com/">
<i class="fa fa-twitter-square"></i>
</a>
</div>
<p class="small">
    
       © Copyright 2022 <i aria-hidden="true" class="fa fa-heart"></i> ngc7293
    
    </p>
<p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
</p>
</footer>
</section>
<script src="https://ngc7292.github.io/js/jquery-2.2.4.min.js"></script>
<script src="https://ngc7292.github.io/js/main.js"></script>
<script src="https://ngc7292.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'ngc7293', 'auto');
	
	ga('send', 'pageview');
}
</script>
</body>
</html>
