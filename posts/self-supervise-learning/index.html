<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		 
			
  
    <meta name="twitter:card" content="summary"/>
    
      <meta name="twitter:image" content="https://ngc7292.github.io/images/avatar.png" />
    
  
  
  <meta name="twitter:title" content="Self-supervised Learning, Generative or Contrastive"/>
  <meta name="twitter:description" content="Self-supervisedLearning: Generativeor Contrastive"/>
  
    <meta name="twitter:site" content="@ngc7293"/>
  
  
  
  
    <meta name="twitter:creator" content="@ngc7293"/>
  



		
		<meta name="author" content="ngc7293">
		
		<meta name="generator" content="Hugo 0.78.1" />
		<title>Self-supervised Learning, Generative or Contrastive &middot; ngc7293&#39;s blog</title>
		<link rel="shortcut icon" href="https://ngc7292.github.io/images/favicon.ico">
		<link rel="stylesheet" href="https://ngc7292.github.io/css/style.css">
		<link rel="stylesheet" href="https://ngc7292.github.io/css/highlight.css">
		<script src="https://cdn.jsdelivr.net/gh/ngc7292/live2d-widget@latest/autoload.js"></script>

		
		<link rel="stylesheet" href="https://ngc7292.github.io/css/font-awesome.min.css">
		

		
		<link href="https://ngc7292.github.io/index.xml" rel="alternate" type="application/rss+xml" title="ngc7293&#39;s blog" />
		

		

		<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://ngc7292.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://ngc7292.github.io/posts'>Archive</a>
	<a href='https://ngc7292.github.io/tags'>Tags</a>
	<a href='https://ngc7292.github.io/about'>About</a>

	

	
	<a class="cta" href="https://ngc7292.github.io/index.xml">Subscribe</a>
	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        Self-supervised Learning, Generative or Contrastive
                    </h1>
                    <h2 class="headline">
                    Oct 31, 2020 00:00
                    · 184 words
                    · 1 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://ngc7292.github.io/tags/nlp">nlp</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                  
                    <div id="toc">
                      <nav id="TableOfContents">
  <ul>
    <li><a href="#自监督学习生成与对比generative-and-contrastive">自监督学习：生成与对比（generative and contrastive）</a>
      <ul>
        <li></li>
        <li><a href="#graph-中的表示学习">graph 中的表示学习</a></li>
        <li><a href="#生成式自监督学习">生成式自监督学习</a></li>
        <li><a href="#对比式自监督学习contastive-self-supervise-learning">对比式自监督学习（contastive self-supervise learning）</a></li>
        <li><a href="#生成对比自监督学习对抗学习">生成对比自监督学习（对抗学习）</a></li>
      </ul>
    </li>
  </ul>
</nav>
                    </div>
                  
                
                <section id="post-body">
                    <p>本文是清华大学的一篇survey <a href="http://arxiv.org/abs/2006.08218">Liu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang, J., Tang, J., &amp; Member, S. (n.d.). <em>Self-supervised Learning : Generative or Contrastive</em>. 1–23.</a>  我来学习一下，顺便记录一下可以看的生成方面的论文。</p>
<h2 id="自监督学习生成与对比generative-and-contrastive">自监督学习：生成与对比（generative and contrastive）</h2>
<p>监督学习在很多地方应用都很多，例如CV，nlp，以及graph representation learning。在nlp中，大部分任务都是监督学习，但是监督学习依赖于手动标记的高质量数据集，对于很多任务来说，手动标注的数据集比较珍贵，而且规模受限制，代价昂贵。而无监督学习与自监督学习可以通过少量的标签样本来得到更多的特征，这篇servey就对自监督学习进行简单的介绍以及整合。</p>
<h4 id="nlp中的表示学习">nlp中的表示学习</h4>
<p>在nlp中，embedding分为带文本信息的（contextual）以及不带文本信息的（non-contextual）的方式，后者只会将token映射到一个空间中，对于$ x \in V $来讲，我们对其进行嵌入得到$ e_x \in \mathbb{R}^d $，对于带文本信息的嵌入，我们可以表示为$ [e_1,e_2,..,e_n] = f(x_1,x_2,&hellip;,x_n) $， f表示embedding函数。contextual embedding中最早的是mikolov的word2vec，cbow，skip-gram，这种方法将每一个token映射为相同的embedding向量，我觉得对于语句的特征并没有提取多少，更类似与n-gram之类的嵌入方法，之后ELMo（Embedding from Language model）出现，一定程度上解决这些问题，目前，BERT（bidirectional Encoder Representations from Transformers）问世之后，在11个nlp任务中得到了巨大的提升。BERT是一个fine tuned approach， 采用transformer作为encoder，有两个任务，其中一个是随机mask部分词语或者token来进行预测，这应该是表示学习中表现最好的一种模型，其表示的信息足够多，特征也足够大，对于很多任务都是相当大的提升。</p>
<h3 id="graph-中的表示学习">graph 中的表示学习</h3>
<p>虽然我们研究的是nlp，但是最近的研究显示，nlp中很多问题都是可以通过graph的结构来进行表示，甚至对于很多问题，graph可以得到的特征以及表示对于普通表示更好一点。</p>
<p>存在一个图，我们可以表示其为$ G = (V,E,X) $V表示的是点集，E表示的是边集，$ X \in \mathbb{R}^{|V|*d} $ 作为原始特征矩阵，通常情况下会设置为正交矩阵或者通过正态分布来进行初始化，在图上的表示学习，问题主要在于如何学习到节点表示：$ Z \in \mathbb{R}^{|V|*d_z} $。类似于NetMF，GraRep，HOPE，DeepWalk，LINE， HARP，以及GCN。</p>
<h3 id="生成式自监督学习">生成式自监督学习</h3>
<h4 id="自回归模型auto-regressivear">自回归模型（auto-regressive，AR）</h4>
<p>自回归模型被看作贝叶斯网络结构，其联合分布可以看作：</p>
<p>$$ max_{\theta}p_\theta(x) = \sum_{t=1}^{T}log \ p_{\theta}(x_t|X_{1:t-1}) $$</p>
<p>在nlp中，自回归模型通常是在正向自回归分解的情况下做最大似然，类似于GPT以及GPT-2等，GPT等是采用transformer decoder结构而与GPT不同，GPT-2并没有采用fine-turing而是通过将各种不同的任务糅合在一起：$ p(output|input,task) $</p>
<p>在图表示学习中，GraphRNN通过深度自回归模型来生成接近真实的图数据，其被看作层级模型，其中，图级别的RNN包括图的state以及生成新的节点，边层级的RNN基于state来更新生成新的边。在此之后还有MRNN，GCPN，通过强化学习的框架对图进行生成。</p>
<h4 id="基于流的模型">基于流的模型</h4>
<p>基于流的模型从数据中生成相关的高维分布$ p(x) $，为了堆叠一系列描述他们的函数逐步生成。总的来说，基于流的模型首先定义一个隐变量z，其满足分布$ p_Z(z) $，之后我们定义一个函数$ z = f_{\theta}(x) $。我们的目标就是找到该函数用于表征$ x $与$ z $之间的关系，因此x与z之间可以表示为$ p_{\theta}(x) = p(f_{\theta}(x))|\dfrac{\partial f_{\theta}(x)}{\partial x}| $，通过这个我们可以得到其似然函数$ max_{\theta}\sum_i log \ p_{\theta}(x^{i}) = max_\theta \sum_i log \ p_Z(f_{\theta}(x))+log| \dfrac{\partial f_{\theta}}{\partial x}(x^{(i)}) | $</p>
<p>该模型的优点在于其两个分布之间的映射是可逆的，但是需要x,z之间必须是相同的分布，并且函数$ f_\theta $需要被设计为可逆并且易于计算，类似于NICE与RealNVP等就是设计了一个映射层来参数化该函数，其中中心创新点在于分割x为$ (x_1,x_2)$ ，并通过在自回归器中应用一个$ (x_1,x_2)$到$ (z_1,z_2) $的一个转换，使得$ z_1 = x_1, z_2 = x_2 + m(x_1) $</p>
<h4 id="自编码模型">自编码模型</h4>
<p>自编码模型的目的是通过输入重新构造一部分输入，由于其灵活性，目前应用比较广泛。</p>
<h5 id="基础自编码模型"><strong>基础自编码模型</strong></h5>
<p>在自编码模型之前出现过的RBM（Restricted Boltzmann Machine）受限玻尔兹曼机，RBM存在两层，一层是可视化层，另一层是作为隐藏层，RBM的目标就是最小化模型的混合分布与数据分布的差距。相反的是，RBM是无向概率图模型，基础自编码模型作为有向概率图模型，并且更容易训练。大体上，自编码器可以看作一个前向神经网络，存在一个encoder $ h = f_{enc}(x) $以及decoder$ x<code> = f_{dec}(h) $其目标是最小化$ x $与$ x</code> $之间的距离，有时自编码器的隐藏层与输入单位相比，可以通过在隐藏单位上施加的系数约束来表示神奇的结构。</p>
<h5 id="内容预测模型cpm"><strong>内容预测模型(CPM)</strong></h5>
<p>CPM是基于输入预测上下文信息。在NLP中，类似于word2vec（CBOW，skip-gram）等都是在embedding中最先启动的自监督学习，继承该类结构，fasttext对子词信息也进行了应用。</p>
<p>在图嵌入方面，deepwalk以及Line都旨在基于最近的点来生成邻居节点。</p>
<h5 id="去噪自编码模型"><strong>去噪自编码模型</strong></h5>
<p>去噪自编码模型的最基本的假设是该编码模型是分布对噪声应该带有鲁棒性，因此我们对其中的一些token进行mask，之后根据上下文关系对mask的token进行预测，BERT是其中最具有代表性的工作。但是这个方法的一个缺点是对下游任务的缺少输入的mask token，因此为了解决这个问题，很多不使用mask而是使用原词或者小概率随机词。</p>
<p>在BERT之后，大家对他的结构进行了各种各样的拓展：Span-bert 选择mask掉随机范围而不是随机token，通过训练范（span）的边界表示来预测被mask的范围，ERINE（baidu）mask掉实体或者短语来学习实体以及短语层次的问题，清华的ERINE模型来使用该方法吧知识应用在语言模型中。</p>
<p>基于图的去噪自编码模型有GPT-GNN以及OAG</p>
<h5 id="变分自编码器"><strong>变分自编码器</strong></h5>
<h4 id="多层生成模型">多层生成模型</h4>
<h5 id="混合arae模型"><strong>混合AR&amp;AE模型</strong></h5>
<h5 id="混合aeflow-based模型"><strong>混合AE&amp;Flow-based模型</strong></h5>
<h3 id="对比式自监督学习contastive-self-supervise-learning">对比式自监督学习（contastive self-supervise learning）</h3>
<h4 id="文本实例对比context-instance-contrast">文本实例对比（context-instance contrast）</h4>
<h4 id="文本相互对比context-context-contrast">文本相互对比（context-context contrast)</h4>
<h3 id="生成对比自监督学习对抗学习">生成对比自监督学习（对抗学习）</h3>
                </section>
            </article>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ngc7293s-blog'; 

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://ngc7293s-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.facebook.com/">
        <i class="fa fa-facebook-square"></i>
    </a>
    
    <a class="symbol" href="https://www.github.com/ngc7292">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.twitter.com/">
        <i class="fa fa-twitter-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2022 <i class="fa fa-heart" aria-hidden="true"></i> ngc7293
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="https://ngc7292.github.io/js/jquery-2.2.4.min.js"></script>
<script src="https://ngc7292.github.io/js/main.js"></script>
<script src="https://ngc7292.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'ngc7293', 'auto');
	
	ga('send', 'pageview');
}
</script>





    </body>
</html>
